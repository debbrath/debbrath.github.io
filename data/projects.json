[
  {
    "id": "1",
    "categories": ["official","software"],
    "image": "./assets/images/project/1.png",
    "title": "Flora EFTN",
    "tags": ["Flora Systems Ltd","ASP.NET","oAuth","JWT","MSSQL Server","Electronic Fund Transfer (EFT)","Banking System","Banking System Integration","BEFTN Outward Automation"],
    "objective": "To modernize the Electronic Fund Transfer (EFT) infrastructure through continuous, automated upgrades, focusing on extensive integration with diverse Core Banking Systems and a broad range of transaction channels to expand service accessibility.",
    "responsibility":"Designed and implemented automated EFT upgrade processes; integrated the system with multiple Core Banking Systems; developed support for diverse transaction channels (Cheque, iBanking, Credit Cards, MFS like UPay/Rocket); built a robust security layer using Ping Federate, JWT, and oAuth; and prepared the infrastructure for a future, bank-wide BEFTN Outward process rollout, eliminating the current manual permission system.",
    "technology":"ASP.NET (Front and Back End), .NET Framework (3.5), Microsoft SQL Server, Crystal Report, SOAP/Web/Windows Services, SFTP, oAuth, Bitbucket/SVN (Source Control), IIS, Windows Server",
    "outcome":"Achieved a highly modern, secure, and integrated EFT system. The project eliminated manual permission requirements for BEFTN Outward and positioned the bank for a full service rollout to all branches, significantly increasing efficiency in fund transfer operations and expanding service reach to more customers.",
    "features": [
      "Automated EFT Upgrade with comprehensive Core Banking System integration.",
      "Support for diverse payment channels (Cheque, iBanking, MFS, Credit Cards).",
      "Robust Security Stack: Ping Federate, JWT, oAuth token-based authentication.",
      "Technologies include ASP.NET, MS SQL, RESTful API, WCF, and SFTP."
    ],
    "link": "https://github.com/Aronno1920"
  }
  ,
  {
    "id": "2",
    "categories": ["official","software"],
    "image": "./assets/images/project/2.png",
    "title": "Niloy Hero",
    "tags": ["Nitol Niloy Group","Vehicle Operations ERP","ASP.NET","SQL Server","Procurement to Pay (P2P)","Workshop","Inventory Management"],
    "objective":"To develop a comprehensive, web-based ERP system on ASP.NET/SQL Server to streamline all core business processes of a vehicle-related company.",
    "responsibility":"Designed and implemented the full Procurement to Pay (P2P) cycle; developed Inventory Management with bin tracking and stock alerts; created the Workshop Management module (Job Card creation, material consumption); and integrated Financial Accounting (bill generation, GL posting).",
    "technology":"ASP.NET (Front and Back End), .NET Framework (4.0), Microsoft SQL Server, Store Procedure, Web Technologies, Crystal Report, Bitbucket/SVN (Source Control), IIS, Windows Server",
    "outcome":"Delivered a centralized ERP solution that significantly reduced operational costs and enhanced productivity by streamlining P2P and workshop processes, providing real-time inventory levels, and ensuring accurate financial reporting.",
    "features": [
      "End-to-End P2P Cycle: From requisition approval to material receipt and vendor payment.",
      "Advanced Inventory Management: Real-time tracking, Bin Management, and automated low-stock alerts.",
      "Integrated Workshop Module: Manages Job Card Creation and automated material consumption tracking.",
      "Financial Automation: Automatic Bill Generation and seamless General Ledger posting."
    ],
    "link": "https://github.com/Aronno1920"
  },
  {
    "id": "3",
    "categories": ["official","software"],
    "image": "./assets/images/project/3.png",
    "title": "Orion POS for Resturant",
    "tags": ["ORION","Personal","ASP.Net MVC","Entity Framework Core","SQL Server"],
    "objective": "To develop a comprehensive, highly flexible, and reliable Point of Sale (POS) software solution designed to manage all core business processes of a restaurant or hospitality operation.",
    "responsibility":"Designed core modules for Order Taking, Table Management, and Billing; implemented integration with Kitchen Display Systems (KDS) and Inventory Tracking; developed a modular system architecture allowing extensive user customization of rules, commands, and screen layouts; and ensured data reliability using a SQL Server backend.",
    "technology":"ASP.Net, C#, Desktop Vesion (POS), Web Application (Admin Panel), Microsoft SQL Server, Kitchen Display Systems (KDS) integration protocols.",
    "outcome":"Delivered a flexible and professional POS system capable of streamlining all aspects of restaurant operations, from order entry to inventory control, leading to enhanced productivity, reduced operational costs, and a customized user experience for management and staff.",
    "features": [
      "Core Operations: Manages all essential restaurant tasks, including order taking, table management, bill printing, inventory tracking, and KDS integration.",
      "Customization: Highly flexible and modular design allows users to customize operational rules, automation commands, and screen layouts.",
      "Versions & Support: Has evolved through several versions (V3-V5); newer versions are commercial and offer professional support.",
      "Database: Typically relies on Microsoft SQL Server for reliable data storage."
    ],
    "link": "https://github.com/Aronno1920"
  },
  {
    "id": "4",
    "categories": ["software", "personal"],
    "image": "./assets/images/project/4.png",
    "title": " ElasticSearch Solutions in .NET CORE",
    "tags": ["Personal","ASP.NET Core",".NET 9","Elasticsearch","Full-Text Search","Data Seeding","Performance Tracking","Entity Framework Core","SQL Server"],
    "objective": "To develop a modern .NET 9 Web API for product management featuring full CRUD operations and dual search functionality using SQL Server and high-performance Elasticsearch.",
    "responsibility":"Implemented full CRUD operations using ASP.NET Core and Entity Framework Core (SQL Server); integrated Elasticsearch via NEST for full-text search; implemented automatic data seeding of 200,000+ products (Bogus); and added performance monitoring middleware and Swagger documentation.",
    "technology":"ASP.NET Core (.NET 9), Entity Framework Core, SQL Server, Elasticsearch, NEST, Bogus, Swagger/OpenAPI.",
    "outcome":"Created a high-performance product API that significantly improved query speed through Elasticsearch integration, ensuring a scalable system with integrated performance tracking and comprehensive API documentation for external developers.",
    "features": [
      ".NET 9 Web API with comprehensive CRUD endpoints.",
      "Dual Search Functionality using SQL Server and Elasticsearch.",
      "Automatic Data Seeding (200,000+ products via Bogus).",
      "Performance Tracking middleware and Swagger documentation."
    ],
    "link": "https://github.com/debbrath/ElasticPracticeSolution"
  },
  {
    "id": "5",
    "categories": ["software", "personal"],
    "image": "./assets/images/project/5.png",
    "title": "eCommerce Application using Microservice Architecture",
    "tags": ["Personal",".NET",".NET Core","Clean Architecture","MongoDB","Redis","PostgreSQL","Ocelot","RabbitMQ","gRPC","Microservices Architecture"],
    "objective": "To build a resilient microservices application using Clean Architecture, integrating polyglot persistence (NoSQL/Relational) and advanced communication (RabbitMQ, gRPC).",
    "responsibility":"Developed key services (Catalog.API with MongoDB, Basket.API with Redis, Discount.API/Grpc with PostgreSQL/Dapper); implemented event-driven communication via RabbitMQ; configured the Ocelot API Gateway for routing; and implemented a robust security stack (Ping Federate, oAuth/JWT).",
    "technology":"ASP.NET, C#, MongoDB, Redis, PostgreSQL, RabbitMQ, Ocelot API Gateway, gRPC, Dapper, Ping Federate, oAuth, JWT, Clean Architecture.",
    "outcome":"Developed a highly scalable and maintainable microservices infrastructure demonstrating expertise in managing complex data structures, high-performance inter-service communication, and enterprise-level security protocols.",
    "features": [
      "Multi-Database Architecture (NoSQL, Relational, Caching).",
      "RabbitMQ for Asynchronous Communication and Ocelot API Gateway.",
      "gRPC (Google Remote Procedure Call) for high-performance service calls.",
      "Clean Architecture principle implemented with C#/.NET."
    ],
    "link": "https://github.com/debbrath/Microservices_Ecommerce"
  },
  {
    "id": "6",
    "categories": ["software", "official"],
    "image": "./assets/images/project/6.png",
    "title": "Employee Assessment System",
    "tags": ["ORION","ASP.Net","MSSQL Server","Role-based KPIs","Job Description Metrics","Functional Performance Assessment","Evaluation","Job-based Performance Indicators (PIs)"],
    "objective": "To build a system that ensures absolute clarity in HR performance evaluation by linking assessment directly to specific job description responsibilities and expected outcomes.",
    "responsibility":"Designed the logic to define and assign unique Performance Indicators (PIs) for each job title; implemented tracking against role-based targets; developed automated performance scoring based on function weightings; and generated cross-role comparison reports.",
    "technology":"ASP.NET (Front and Back End), .NET Framework (4.0), Microsoft SQL Server, Store Procedure, Web Technologies, Crystal Report, Bitbucket/SVN (Source Control), IIS, Windows Server",
    "outcome":"Created a highly transparent and objective appraisal system that successfully drives role-specific excellence by measuring employees against clear, relevant, and fair benchmarks, significantly streamlining the HR appraisal process.",
    "features": [
      "Unique PIs defined and assigned for every specific job title.",
      "Performance metrics linked directly to core duties in job descriptions.",
      "Automated performance scoring based on job function weightings.",
      "Reporting features to compare performance across similar roles."
    ],
    "link": "https://www.linkedin.com/in/aronno1920/"
  },
  {
    "id": "7",
    "categories": ["software", "official"],
    "image": "./assets/images/project/7.png",
    "title": "AI-Powered Online Exam System",
    "tags": ["ORION","MAUI","ASP.Net","MSSQL Server","Digital Examination Platform","AI-Powered Assessment","Question Bank Management","Online Examination System","MCQ Auto-Grading", "AI Narrative Grading", "Role-Based Marking", "Automated Assessment", "NLP for Exams"],
    "objective": "To develop an advanced, secure, and user-friendly web-based platform to revolutionize the entire examination and assessment process for organizations.",
    "responsibility":"Developed core modules for Online Registration and Question Bank Management; implemented secure exam delivery; integrated MCQ Auto-Marking; created a Role-Based Narrative Marking panel; and deployed AI-Generated Auto Narrative Marking using NLP and machine learning.",
    "technology":"MAUI (Client), ASP.Net (Admin Panel), .NET Framework (4.5), Microsoft SQL Server, Store Procedure, Web API, Crystal Report, Github (Source Control), IIS, Windows Server, AI/ML, Natural Language Processing (NLP)",
    "outcome":"Delivered a complete digital assessment solution that ensures high efficiency, fairness, and security. The system provides deep, immediate insights into candidate performance through automated and cutting-edge AI-powered evaluation.",
    "features": [
      "Online Registration handles secure candidate sign-ups, fee payments, and document uploads.",
      "Question Bank Management offers a centralized, rich, and searchable repository for all question types.",
      "Exam Scheduling & Creation allows easy configuration of exam parameters, duration, and rules like negative marking.",
      "Secure Exam Delivery prevents cheating through randomization, time limits, and optional proctoring integration.",
      "MCQ Auto-Marking instantly and automatically grades multiple-choice questions upon submission.",
      "Role-Based & AI Narrative Marking enables human evaluators and AI to score and provide feedback on subjective, long-form answers."
    ],
    "link": "https://www.linkedin.com/in/aronno1920/"
  },
  {
    "id": "8",
    "categories": ["application", "official"],
    "image": "./assets/images/project/8.png",
    "title": "Orion Portal",
    "tags": ["ORION","Android","Java",".Net Core API","HR & Employee Self-Service (ESS)","Smart Attendance Tracking","Mobile Leave Management","Payslip/Performance App"],
    "objective": "To develop an all-in-one HR & Employee Self-Service (ESS) Android application to manage work seamlessly and keep employees productive while on the move.",
    "responsibility":"Developed core features including Smart Attendance Tracking, the Leave Application & Approval module, and Out-Station Adjustment submission; integrated secure access to Payslips and salary breakdowns; and implemented a system for company-wide notifications.",
    "technology":"Android App Development, Java Native, API Integration.",
    "outcome":"Empowered the workforce with instant, secure mobile access to critical HR services, significantly improving employee productivity, enhancing communication, and reducing the administrative workload on the HR department.",
    "features": [
      "Smart Attendance Tracking (Anytime, Anywhere marking).",
      "Comprehensive Leave Management (Application, Tracking, Approval).",
      "Instant access to Payslips, Salary Breakdown, and Performance Updates.",
      "Secure Employee Self-Service (ESS) portal with company notifications."
    ],
    "link": "https://www.linkedin.com/in/aronno1920/"
  },
  {
    "id": "9",
    "categories": ["software", "official"],
    "image": "./assets/images/project/9.png",
    "title": "E-Document Tracking System",
    "tags": ["ORION","Android","Java",".Net Core API","HR & Employee Self-Service (ESS)","Smart Attendance Tracking","Mobile Leave Management","Payslip/Performance App"],
    "objective": "To develop a secure, intelligent, and scalable web-based Document Management System (DMS) to manage the complete document lifecycle from creation to archival.",
    "responsibility":"Established a Centralized Document Repository with advanced indexing; implemented a multi-level Role-Based Approval System with digital signatures; developed Workflow Automation for routing; implemented stringent Version Control and Audit Trails; and ensured Security & Compliance (encryption, GDPR/HIPAA).",
    "technology":"ASP.NET (Front and Back End), .NET Framework (4.5), Microsoft SQL Server, Web Technologies, Crystal Report, Github (Source Control), ERP/HRMS/CRM Integration APIs.",
    "outcome":"Delivered a comprehensive solution that significantly enhanced document security and compliance, improved productivity through workflow automation, and provided reliable audit-readiness while reducing paperwork.",
    "features": [
      "Centralized Repository with Store and organize files in secure, advanced search and metadata indexing.",
      "Role-Based Approval System with digital signatures and audit logs.",
      "Seamless Collaboration with Share documents, add comments, and work together in real time.",
      "Workflow Automation for routing, reminders, and escalation tracking.",
      "Version Control & Audit Trail with Track revisions, maintain history, and ensure compliance."
    ],
    "link": "https://www.linkedin.com/in/aronno1920/"
  },
  {
    "id": "10",
    "categories": ["software", "official"],
    "image": "./assets/images/project/10.png",
    "title": "Vehicle Management System",
    "tags": ["ORION","ASP.Net","MSSQL Server","VMS","Vehicle Management System","Fleet Management Software","Fuel Tracking","Maintenance Tracking","Compliance Automation"],
    "objective":"To develop a comprehensive digital platform to automate organizational vehicle operations, focusing on efficient fleet management, cost control, and regulatory compliance.",
    "responsibility": "Implemented the Vehicle Requisition & Assignment workflow; developed detailed Fuel Requisition & Bill Adjustment tracking; implemented Automated Notifications for compliance documents (Registration, Insurance); and created the Vehicle Maintenance Records module.",
    "technology":"ASP.NET (Front and Back End), .NET Framework (4.0, 4.5), Microsoft SQL Server, Web Technologies, Crystal Report, Github (Source Control)",
    "outcome":"Created an efficient fleet management platform that reduced operational costs (especially fuel wastage), ensured 100% compliance via automated alerts, and prolonged vehicle lifespan through structured preventive maintenance tracking.",
    "features": [
      "Streamlined Requisition and transparent vehicle assignment workflow.",
      "Fuel Management (Requisition, Consumption Log, and Bill Adjustment).",
      "Automated Compliance Alerts for registration, tax token, and insurance.",
      "Structured Maintenance Records for tracking costs and scheduling preventive care.",
      "Data-driven insights for efficient fleet management and cost savings, Streamlined vehicle requisition and assignment process."
    ],
    "link": "https://www.linkedin.com/in/aronno1920/"
  },
  {
    "id": "11",
    "categories": ["application", "official"],
    "image": "./assets/images/project/11.png",
    "title": "Prescription Mind to Heart",
    "tags": ["ORION","Android","Java",".Net Core API","Field Force Mobile Application","Medical Sales","Pharma Tech","Digital Brochure Sharing","Knowledge Hub","Real-Time Notifications"],
    "objective": "To develop a powerful mobile solution to empower medical representatives and sales teams (field staff) with tools for productivity, information, and communication on the move.",
    "responsibility":"Developed core modules for access and sharing of Medical Promotion & Campaigns materials; implemented Digital Brochure Sharing ('Brushier Share') with activity tracking; integrated Real-Time Notifications; and created a Medical Articles & Knowledge Hub with offline access.",
    "technology":"Android App Development, Java Native, API Integration.",
    "outcome":"Significantly increased field force productivity and professionalism by providing instant, digital access to up-to-date knowledge and promotional materials, resulting in stronger customer engagement and reduced printing costs.",
    "features": [
      "Medical Promotion & Campaign material access and sharing.",
      "Digital Brochure Sharing ('Brushier Share') with tracking features.",
      "Real-Time Notifications for urgent updates and policy changes.",
      "Knowledge Hub for continuous learning via medical articles and research."
    ],
    "link": "https://www.linkedin.com/in/aronno1920/"
  },
  {
    "id": "12",
    "categories": ["application", "official"],
    "image": "./assets/images/project/12.png",
    "title": "Orion Scholars",
    "tags": ["ORION","Android","Java",".Net Core API","CSR Scholarship Management","Mobile","Admin Panel","Financial Disbursement","Tracking","Student Database","Cheque Reconciliation"],
    "objective":"To streamline Orion's CSR scholarship management process using a dedicated Android application and integrated admin panel, ensuring transparency and efficiency.",
    "responsibility":"Developed the Admin Panel for Scholarship Student Management; implemented Month-Wise Scholarship Setup; managed the full disbursement process including Bank Cheque Update/Reconciliation; and created the Student App for alerts and digital fund receipt confirmation.",
    "technology":"Android Application Development, Java, Web-based Admin Panel, Notification Services, API Intregration, Microsoft SQL Server",
    "outcome":"Established a highly transparent and efficient digital scholarship management system that ensures CSR accountability through accurate budget tracking, minimizes administrative errors in disbursement, and provides a reliable user experience for recipients.",
    "features": [
      "CSR Scholarship Management via integrated Admin Panel and Android App.",
      "Month-Wise Scholarship Setup and Bank Cheque Disbursement tracking.",
      "Budget Management with detailed cost tracking for accountability.",
      "Digital Confirmation and Real-Time Notifications for students."
    ],
    "link": "https://www.linkedin.com/in/aronno1920/"
  },
  {
    "id": "13",
    "categories": ["software", "official"],
    "image": "./assets/images/project/13.png",
    "title": "SAP | MM Module | OHAL | POWER",
    "tags": ["ORION","SAP","MM Module","OHAL","POWER","Data Preparation","User Training","Implementation","SAP Material Management (MM)","Procurement to Pay","Master Data Preparation","Inventory Control","Stock Accounting"],
    "objective": "This comprehensive program provides an in-depth exploration of the SAP Material Management (MM) module, focusing on the entire Procurement to Pay (P2P) cycle within the broader SAP landscape. It covers core MM Business Concepts and the essential preparation of Master Data (Material, Vendor, Service).",
    "responsibility":"Executed all steps in the P2P cycle, including Master Data Preparation (Material, Vendor); processing Purchase Requisition, Quotation, and various Purchasing types; managed Goods Receipt and Incoming Quality Control; and finalized the process with Inventory Control and Vendor Bill Settlement.",
    "technology":"SAP ERP, SAP Material Management (MM) Module.",
    "outcome":"Achieved comprehensive proficiency in managing the end-to-end P2P lifecycle within SAP, enabling streamlined supply chain operations, accurate inventory management, and effective vendor bill settlement processes.",
    "features": [
      "To master and apply the core concepts and processes of the SAP Material Management (MM) module, focusing on the entire Procurement to Pay (P2P) lifecycle.",
      "Focus on Master Data Preparation (Material, Vendor, Service) and Business Concepts.",
      "Covers all core transactions: Requisition, Purchasing, Goods Receipt, and Bill Settlement.",
      "Includes advanced topics like Inventory Control, Stock Accounting, and Material Loan Process."
    ],
    "link": "https://www.linkedin.com/in/aronno1920/"
  },
  {
    "id": "14",
    "categories": ["software", "official"],
    "image": "./assets/images/project/14.png",
    "title": "Supply Chain Management System (SCM)",
    "tags": ["ORION","ASP.Net","MSSQL Server","Supply Chain Management (SCMS)","Procurement","MRP","Inventory Management","Warehouse Management","Logistics & Distribution","Three-Way Match"],
    "objective": "To develop a comprehensive digital platform that integrates and manages the end-to-end supply chain (procurement, inventory, logistics, production) with high visibility and control.",
    "responsibility":"Developed Procurement & Vendor Management workflows; implemented Inventory & Warehouse Management (centralized stock, automated replenishment); created Logistics & Distribution features (route optimization, tracking); and integrated Financial Controls using a Three-Way Match system.",
    "technology":"ASP.NET (Front and Back End), .NET Framework (4.0, 4.5), Microsoft SQL Server, Web Technologies, Crystal Report, Github (Source Control), Logistics APIs.",
    "outcome":"Delivered an integrated SCMS that achieved end-to-end visibility, realized significant cost optimization through smarter planning and three-way matching, and enhanced overall regulatory compliance and operational efficiency.",
    "features": [
      "End-to-End Supply Chain Integration (Procurement, Inventory, Logistics).",
      "Automated Stock Replenishment and Batch/Lot Tracking.",
      "Logistics Optimization with route planning and delivery scheduling.",
      "Financial Control via Three-Way Match and automated MRP."
    ],
    "link": "https://www.linkedin.com/in/aronno1920/"
  },
  {
    "id": "15",
    "categories": ["software", "official"],
    "image": "./assets/images/project/15.png",
    "title": "Website Development",
    "tags": ["ORION","Wordpress","CodeIgniter","Php","mySQL","Custom WordPress Development","Multi-Business Unit Website","Responsive Web Design","Scalable Content Management"],
    "objective": "To develop a custom, modern, and responsive WordPress-based website to unify the digital presence for multiple, diverse Orion business units (Gym, Restaurant, Infusion Ltd.).",
    "responsibility":"Designed and developed a Custom WordPress Theme ensuring responsive design; created and integrated dedicated functional sections for Orion Gym (booking) and Orion Restaurant (reservations); optimized the platform for SEO, security, and performance; and set up the Content Management System backend.",
    "technology":"WordPress, Custom WordPress Theme Development, HTML5, CSS, PHP, JavaScript, SEO Tools.",
    "outcome":"Successfully unified diverse business ventures under a single, scalable digital umbrella, resulting in an engaging customer experience, enhanced brand consistency, and a foundation for future business unit integration.",
    "features": [
      "Unified Digital Platform managing multiple diverse business units.",
      "Custom WordPress Theme ensuring consistent branding and responsive design.",
      "Dedicated modules for Gym (Booking), Restaurant (Reservations), and Corporate Display.",
      "Optimized for SEO, performance, and security enhancements."
    ],
    "link": "https://www.linkedin.com/in/aronno1920/"
  }



  ,
  {
    "id": "50",
    "categories": ["application", "personal"],
    "image": "./assets/images/project/50.png",
    "title": "TechAid24",
    "tags": ["Personal","TechAid24","Blog Site", "Technical Blog","AI related Bangla Blog"],
    "objective": "To establish an independent online technical support platform and publication dedicated to empowering learners, developers, and tech enthusiasts with practical tutorials.",
    "responsibility":"Developed and maintained the online publication platform; created and published practical tutorials and troubleshooting guides across key technology domains (Android, Web Dev, SQL); managed community interaction; and provided personalized assistance to visitors.",
    "technology":"Online Publication Platform (CMS/Custom), Web Technologies, SQL, Expertise in multiple programming fields.",
    "outcome":"Successfully built a supportive and credible online resource (TechAid24) that simplifies complex technology topics, provides real-world solutions, and fosters a growing community of learners and developers through high-clarity content and responsive engagement.",
    "features": [
      "Independent Online Platform for technical support and publication.",
      "Specializes in Android, Web Dev, SQL, and Troubleshooting tutorials.",
      "Expert Team committed to prompt responses and personalized guidance.",
      "Mission: Simplify technology and provide real-world solutions openly."
    ],
    "link": "https://techaid24.com"
  },
  {
    "id": "51",
    "categories": ["ai-solutions", "personal"],
    "image": "./assets/images/project/51.png",
    "title": "Diabetes Detection System",
    "tags": ["Personal","Binary Classification","Diabetes Prediction","Logistic Regression","KNN","Decision Tree","Random Forest","Stochastic Gradient Descent","Gradient Boosting","AdaBoost","Extra Trees","Support Vector Machine","Naive Bayes","ROC AUC","Feature Scaling","Machine Learning"],
    "objective": "To develop a robust binary classification model capable of predicting the presence or absence of diabetes based on a medical features dataset, with a focus on implementing and comparatively evaluating multiple machine learning classifiers.",
    "responsibility":"Split the dataset into training and testing sets; applied feature scaling (e.g., using StandardScaler); implemented and trained at least three distinct classifiers (Logistic Regression, KNN, Decision Tree); rigorously evaluated and compared each model's performance; and generated visualizations including the ROC Curve and Confusion Matrix.",
    "technology":"Python, Machine Learning Library (Scikit-learn implied), Pandas, Matplotlib/Seaborn, Logistic Regression, K-Nearest Neighbors (KNN), Decision Tree, StandardScaler.",
    "outcome":"Successfully identified the optimal model for diabetes prediction (based on the highest ROC AUC and F1-score), demonstrating proficiency in classification workflows, comparative analysis, and advanced performance evaluation crucial for healthcare applications.",
    "features": [
      "Binary Classification: Predicts diabetes status (0 or 1) from medical features.",
      "Model Comparison: Evaluates performance across Logistic Regression, KNN, and Decision Trees.",
      "Essential Preprocessing: Mandatory use of Feature Scaling to normalize input data ranges.",
      "Rigorous Evaluation: Uses Confusion Matrix, P/R/F1-score, and ROC AUC for performance assessment."
    ],
    "link": "https://colab.research.google.com/drive/1grLlQDlWzbQRq1Mnhkl9YH1rZk6LhiLF#scrollTo=rVqAjWQg19ip"
  },
  {
    "id": "52",
    "categories": ["ai-solutions", "personal"],
    "image": "./assets/images/project/52.png",
    "title": "Clustering Assignment using Iris Dataset",
    "tags": ["Personal","Clustering Algorithms","K-Means","DBSCAN","Hierarchical","Iris Dataset","StandardScaler","Unsupervised Learning"],
    "objective": "To apply and compare three distinct unsupervised clustering algorithms (K-Means, Hierarchical, DBSCAN) to group the flowers of the Iris dataset, solely using the four continuous features, to understand their effectiveness in finding natural groupings.",
    "responsibility":"Performed Data Preprocessing (feature scaling using StandardScaler); conducted Exploratory Data Analysis (EDA); systematically applied K-Means (using the Elbow Method to find optimal k), Hierarchical Clustering (using a Dendrogram), and DBSCAN (using a k-distance graph to find eps); and visualized the resulting clusters (e.g., via PCA).",
    "technology":"Scikit-learn, Pandas, Matplotlib/Seaborn (for EDA and visualization), StandardScaler, PCA (for visualization).",
    "outcome":"Successfully performed a comparative analysis of partition-based, density-based, and hierarchical clustering techniques. The project demonstrates proficiency in unsupervised learning methodology, optimal parameter selection, and the use of visualization to interpret cluster quality and structure.",
    "features": [
      "Comparative Clustering: Directly compares K-Means, Hierarchical, and DBSCAN algorithms.",
      "Unsupervised Preprocessing: Applied to the Iris Dataset using StandardScaler for feature scaling.",
      "Parameter Optimization: Uses the Elbow Method (K-Means) and Dendrogram (Hierarchical) to select optimal parameters.",
      "Insight Generation: Includes EDA and Visualization to assess the final cluster groupings."
    ],
    "link": "https://colab.research.google.com/drive/1mAW9mE-_2IIrZXnF16ybmFjss-2oJW_X?usp=sharing"
  },
  {
    "id": "53",
    "categories": ["ai-solutions", "personal"],
    "image": "./assets/images/project/53.png",
    "title": "Digit Classifier (MNIST)",
    "tags": ["Personal","MNIST Classifier","Batch Normalization","Dropout Regularization","Keras","Deep Learning","Deep Learning Architecture"],
    "objective": "To build and evaluate a Deep Neural Network (DNN) for classifying handwritten digits using the MNIST dataset, with a core focus on reinforcing essential deep learning concepts like data preprocessing and regularization techniques.",
    "responsibility":"Performed necessary data preprocessing (Normalization and Flattening of 28x28 images); designed the DNN architecture incorporating Batch Normalization (for faster convergence) and Dropout layers (to mitigate overfitting); compiled and trained the model (e.g., using categorical_crossentropy and adam); and generated visual plots comparing training and validation accuracy/loss over epochs.",
    "technology":"Deep Learning Framework (TensorFlow/Keras, implied), MNIST Dataset, Python, Dense Layers, Batch Normalization, Dropout Layers.",
    "outcome":"Successfully built and optimized a high-accuracy DNN for digit classification, demonstrating practical application of key deep learning regularization techniques, resulting in a reliable model and clear visualization of its learning behavior.",
    "features": [
      "DNN Architecture: Built a Deep Neural Network for the Handwritten Digit Classification task.",
      "Essential Regularization: Mandatory use of Batch Normalization and Dropout layers for model stability.",
      "Data Preparation: Includes essential preprocessing steps like Normalization and Flattening the input data.",
      "Performance Reporting: Visual comparison of Training vs. Validation Accuracy and final test accuracy report."
    ],
    "link": "https://github.com/debbrath/Digit-Classifier-MNIST"
  },
  {
    "id": "54",
    "categories": ["ai-solutions", "personal"],
    "image": "./assets/images/project/54.png",
    "title": "Image Classification with NN vs CNN",
    "tags": ["Personal","Image Classification","NN","CNN","NN vs CNN","Fashion MNIST","TensorFlow","Flask","FastAPI","Deep Learning","Deep Learning Architecture"],
    "objective": "To conduct a comparative performance analysis between a standard Neural Network (NN) and a Convolutional Neural Network (CNN) on the Fashion MNIST dataset, explicitly highlighting the architectural superiority of CNNs for visual classification tasks.",
    "responsibility":"Preprocessed the 28x28 grayscale images (normalization, flattening for NN, and reshaping for CNN); designed and built both the fully-connected NN and the CNN (using Conv2D and MaxPooling layers); trained both models on the dataset; and generated final outputs including test accuracy and visualized sample predictions for a quantified comparison.",
    "technology":"Deep Learning Framework (TensorFlow/Keras, implied), Python, Fashion MNIST Dataset, Dense Layers, Conv2D Layers, MaxPooling Layers.",
    "outcome":"Provided empirical evidence demonstrating the significant performance advantage of feature extraction via CNN layers over dense layers for image data, reinforcing core concepts in Deep Learning Architecture and improving model accuracy for garment classification.",
    "features": [
      "Comparative Model Study: Directly contrasts a basic NN against a powerful CNN architecture.",
      "Fashion MNIST Classification: Solves a 10-category image classification problem.",
      "Data Preparation: Essential preprocessing includes normalization and model-specific reshaping (flattening vs. 28x28x1).",
      "Quantified Comparison: Displays test accuracy and visualized sample predictions to prove CNN efficacy."
    ],
    "link": "https://github.com/debbrath/Fashion-Classifier-MNIST"
  },
  {
    "id": "55",
    "categories": ["ai-solutions", "personal"],
    "image": "./assets/images/project/55.png",
    "title": "Object Tracking & Heatmap Visualization",
    "tags": ["Personal","People Flow Detection","Object Tracking (ByteTrack/SORT)","ByteTrack","SORT","YOLO Detection","Heatmap Visualization","Counting"],
    "objective": "To build a Computer Vision system that integrates object tracking, motion counting, and heatmap generation to monitor and quantify the flow (IN/OUT) of people across a defined area in a video feed.",
    "responsibility":"Implemented a YOLO model for robust person detection; integrated a high-performance Object Tracker (e.g., ByteTrack) to assign and maintain unique person IDs; developed custom logic to define and monitor movement across two horizontal lines; calculated the IN/OUT directional counts; and generated a final Heatmap visualizing areas of high presence/motion intensity.",
    "technology":"YOLO (for detection), ByteTrack / DeepSORT / SORT (for tracking), Python, OpenCV (for video/visuals), Roboflow PolygonZone (for defining lines).",
    "outcome":"Created an automated system for retail analytics, security, or traffic monitoring that provides real-time, quantitative metrics (IN/OUT counts) and spatial insights (heatmap) into human movement patterns, enhancing operational and security assessments.",
    "features": [
      "Object Detection and Tracking: Uses YOLO for detection and a high-performance tracker (ByteTrack/DeepSORT) to maintain unique IDs.",
      "Directional Flow Counting: Implements specific IN/OUT logic based on crossing two horizontal lines with defined directionality.",
      "Spatial Visualization: Generates a Heatmap to visualize areas of highest person presence or motion intensity.",
      "Real-time Output: Provides live counters and bounding boxes with unique IDs overlaid on the video."
    ],
    "link": "https://colab.research.google.com/drive/1LrghvQSfU4WAIVVYd-zd7vl_-4t-06CF?usp=sharing"
  },
  {
    "id": "56",
    "categories": ["ai-solutions", "personal"],
    "image": "./assets/images/project/56.png",
    "title": "Model & APIs Deployment on Render",
    "tags": ["Personal","FastAPI Deployment","Docker","Machine Learning API","Render Cloud Host","Model","Deployement"],
    "objective": "To build, containerize (Docker), and deploy a FastAPI web service serving predictions from a machine learning classifier for Heart Disease detection.",
    "responsibility":"Trained and persisted a basic machine learning model (e.g., Logistic Regression); developed the FastAPI application with Pydantic schemas and necessary endpoints (/predict, /health); wrote the Dockerfile and docker-compose.yml; and successfully deployed the containerized service to a cloud host (Render).",
    "technology":"FastAPI, Pydantic, Docker, Docker-Compose, Joblib, Scikit-learn, Render (Cloud Host).",
    "outcome":"Successfully achieved a continuous deployment workflow for an ML model, demonstrating proficiency in MLOps by creating a scalable, production-ready microservice accessible via a well-documented REST API.",
    "features": [
      "FastAPI serving predictions from a Heart Disease Classifier.",
      "Pydantic schema validation for all input features.",
      "Dockerization for consistent, reproducible environments.",
      "Cloud Deployment (e.g., Render) of the containerized application."
    ],
    "link": "https://github.com/debbrath/Predict-Heart-Disease"
  },
  {
    "id": "57",
    "categories": ["ai-solutions", "personal"],
    "image": "./assets/images/project/57.png",
    "title": "Sentiment Analysis using IMDB Dataset",
    "tags": ["Personal","Sentiment Analysis","Text Vectorization","IMDB","TF–IDF","Word2Vec","BERT","NLP Classification"],
    "objective": "To compare the performance of sentiment classification models on the IMDB dataset using three different text representation (vectorization) methods.",
    "responsibility":"Loaded and preprocessed the IMDB dataset; implemented and trained separate classifiers using TF-IDF, Word2Vec embeddings, and BERT embeddings; evaluated all models on the test set; and created a comparative report detailing performance trade-offs (accuracy, time, resources).",
    "technology":"TF-IDF, Word2Vec, BERT embeddings, Classification Models (e.g., Logistic Regression), Hugging Face/TensorFlow Datasets, Google Colab.",
    "outcome":"Conducted a comprehensive analysis that provided data-driven insights into how different NLP feature engineering approaches impact classification accuracy, memory usage, and training time.",
    "features": [
      "Sentiment Analysis on the IMDB Reviews Dataset.",
      "Comparative study of TF–IDF, Word2Vec, and BERT embeddings.",
      "Evaluation using Accuracy, Precision, Recall, and F1 Score.",
      "Focus on Text Representation impact on classification performance."
    ],
    "link": "https://github.com/debbrath/Sentiment-Analysis"
  },
  {
    "id": "58",
    "categories": ["ai-solutions", "personal", "official"],
    "image": "./assets/images/project/58.png",
    "title": "Fine Tuning Transformers for Question Answer",
    "tags": ["Transformer Fine-Tuning","Question Answering (QA)","Hugging Face","SQuAD","BERT","Exact Match (EM)"],
    "objective": "To fine-tune a pre-trained BERT-based model on the SQuAD v1.1 dataset to perform the Question Answering (QA) task, demonstrating proficiency in the Hugging Face ecosystem.",
    "responsibility":"Loaded and preprocessed the SQuAD dataset (tokenization, answer mapping); loaded the bert-base-uncased model; fine-tuned the model using the Hugging Face Trainer API; and evaluated performance using Exact Match (EM) and F1 Score metrics.",
    "technology":"Hugging Face transformers, datasets, BERT-base-uncased, SQuAD v1.1, Google Colab.",
    "outcome":"Successfully trained a state-of-the-art model for extractive QA, achieving measurable performance and demonstrating core competencies in advanced deep learning model fine-tuning and evaluation for NLP.",
    "features": [
      "Transformer Fine-Tuning for Question Answering (QA).",
      "Uses BERT-base-uncased on the SQuAD v1.1 dataset.",
      "Implementation via the Hugging Face datasets and transformers APIs.",
      "Evaluation using Exact Match (EM) and F1 Score metrics."
    ],
    "link": "https://colab.research.google.com/drive/1cHJMaC-yK7Ug1CEeaK7cX7dA61agQ4y1#scrollTo=AvCHHO7AmqLB"
  },
  {
    "id": "59",
    "categories": ["ai-solutions", "personal"],
    "image": "./assets/images/project/59.png",
    "title": "AI Agent for LinkedIn Post Generator",
    "tags": ["Personal","LangChain Agent","LinkedIn Post Generator","LLM Chain","Content Generation","Multilingual AI","GitHub Models", "Lovable"],
    "objective": "To develop a LangChain AI Agent that generates professional, structured LinkedIn posts based on a user-provided topic and language.",
    "responsibility":"Designed and built the AI Agent using LangChain; configured the Large Language Model (LLM) chain to accept and process multilingual inputs; ensured the generated output was an engaging, platform-specific post (2–4 paragraphs).",
    "technology":"LangChain, LLM (Large Language Model), AI Agent.",
    "outcome":"Developed a scalable content creation tool that automates the generation of multilingual, professional social media content, ensuring platform-specific quality and increasing user productivity.",
    "features": [
      "LangChain AI Agent for content generation.",
      "Multilingual Post Creation based on user-specified language.",
      "LinkedIn-Optimized Output (professional, engaging structure).",
      "Input variables: Topic and Language."
    ],
    "link": "https://github.com/debbrath/PostGenerator-AIAgent-LangChain"
  },
  {
    "id": "60",
    "categories": ["ai-solutions", "personal"],
    "image": "./assets/images/project/60.png",
    "title": "AI Agent Project with n8n",
    "tags": ["Personal","n8n Workflow Automation","AI Article Summarization","FastAPI & Webhooks","No-Code","Low-Code AI","Gemini Models", "Lovable"],
    "objective": "To build a full-stack, automated AI workflow system using n8n to process article URLs submitted via a Frontend/FastAPI Backend.",
    "responsibility":"Configured the FastAPI backend to send user input to an n8n webhook; developed the core n8n workflow for web scraping (Firecrawl), generating AI summaries and key insights (OpenAI); automated data persistence to Google Sheets; and implemented the final email delivery of results to the user.",
    "technology":"n8n Workflow Automation, FastAPI, Webhooks, Firecrawl, OpenAI Node, Google Sheets, Email Tool.",
    "outcome":"Delivered an automated, low-code solution that drastically reduced manual article analysis time by instantaneously scraping, processing, analyzing, archiving, and communicating results to the end-user.",
    "features": [
      "Full-Stack Workflow: Frontend, FastAPI Backend, and n8n orchestration.",
      "AI-Powered Article Analysis: Summarization and key insight extraction.",
      "Automated Data Flow: Webhook trigger, data persistence to Google Sheets, and user email delivery.",
      "n8n Automation: Used for all core intelligence and tool integration (Firecrawl, OpenAI, Google Sheets, Email)."
    ],
    "link": "https://github.com/Aronno1920"
  },
  {
    "id": "61",
    "categories": ["ai-solutions", "personal", "official"],
    "image": "./assets/images/project/61.png",
    "title": "Multitools Agent Medical Queries",
    "tags": ["Multitools AI Agent","Agent Routing","SQLite","LangChain", "GitHub Models", "Streamlit"],
    "objective": "To build a Multi-Tool AI Agent capable of answering data-specific queries from medical datasets and retrieving general medical knowledge using web search.",
    "responsibility":"Built the main AI Agent logic for intelligent query routing; converted three medical datasets (Heart Disease, Cancer, Diabetes) into dedicated SQLite DBs; developed three DB-specific Langchain Agents (Tools); and integrated a Medical Web Search Tool.",
    "technology":"OpenAI Agent SDK, Langchain Agent Executor, SQLite, SQL, Web Search API (SerpAPI/Tavily/Bing).",
    "outcome":"Created an intelligent system that unified access to both structured data (for statistics) and general knowledge (for definitions), delivering complex medical query results in a single, natural language response.",
    "features": [
      "Intelligent Query Routing to select the best knowledge source.",
      "Three Dedicated Database Tools for querying medical statistics via SQL.",
      "Integration of a Web Search Tool for general, up-to-date medical knowledge.",
      "Uses OpenAI Agent SDK and Langchain Agent Executor for robust logic.",
      "Mixed Queries → Both database and web search tools"
    ],
    "link": "https://github.com/debbrath/Multi-Tool-Agent"
  },
  {
    "id": "62",
    "categories": ["ai-solutions", "personal"],
    "image": "./assets/images/project/62.png",
    "title": "CrewAI for Instagram Content Creation",
    "tags": ["Personal","CrewAI","CrewAI Tools","LangChain Community", "GitHub Models", "Multi-Agent","Multi-Agent System", "Image Generation", "Instagram Content Creation"],
    "objective": "To design and implement a Multi-Agent System using Crew AI to fully automate the end-to-end pipeline for Instagram content creation, from initial topic research to final image generation.",
    "responsibility":"Designed and orchestrated a 4-agent team: Research Agent (information gathering), Content Writer Agent (caption/CTA drafting), Reviewer Agent (editing/approval), and Image Prompt Generator Agent (detailed text-to-image prompts); integrated agents using the Crew AI framework to ensure sequential and collaborative workflow; and connected to an external text-to-image API for final visual asset creation.",
    "technology":"Crew AI (Framework), LLM (e.g., OpenAI/Anthropic/other models for agents), External Text-to-Image API (Nano Banana, Segmind, Stable Diffusion, etc.), Python, Research Tools (Web search/scrapers accessible by the Research Agent).",
    "outcome":"Delivered a complete, automated content package (caption, hashtags, 2-3 images) for any given topic. The system demonstrates advanced AI workflow automation, achieving efficiency and consistency in digital marketing content creation by mimicking a human team structure.",
    "features": [
      "4-Agent Orchestration: Implements a collaborative team (Researcher, Writer, Reviewer, Prompt Generator).",
      "Crew AI Framework: Used to define roles, goals, and manage the sequential workflow.",
      "Full-Cycle Automation: Automates content generation from research/writing to final image creation.",
      "External API Integration: Connects the text-based workflow to an image generation service for visual output."
    ],
    "link": "https://github.com/debbrath/Multi-Tool-Agent"
  }  ,
  {
    "id": "63",
    "categories": ["ai-solutions", "personal", "official"],
    "image": "./assets/images/project/63.png",
    "title": "Bangla FAQ Chatbot",
    "tags": ["Bangla","Bangla FAQ","FAQ Chatbot", "RAG", "HuggingFace Model", "GitHub Models", "Multi-Agent"],
    "objective": "Develop an interactive Bengali FAQ Chatbot using the RAG architecture to demonstrate proficiency in core AI and web integration technologies. Key requirements include: Bengali NLP, FAISS vector retrieval with metadata filtering, and full-stack integration using Flask/Gemini.",
    "responsibility":"Create a Bengali FAQ dataset (5 topics, ≥3 FAQs each), tag with metadata (topic/difficulty), and initialize FAISS using the Bengali SBERT model. Implement a Flask server to manage user sessions. Execute FAISS similarity search incorporating dynamic metadata filtering. Use the retrieved context to generate answers via the Gemini API. Build a responsive UI (index.html) using Tailwind CSS. Ensure interactive button-based menus for topic and difficulty filtering, and implement a Bengali fallback mechanism.",
    "technology":"RAG, FAQ Chatbot, HuggingFace Model, GitHub Model, Python.",
    "outcome":"A functional RAG web application that answers Bengali queries based on the filtered FAQ data. A clean, modular Python codebase (app.py, rag_core.py) demonstrating advanced RAG techniques. A comprehensive requirements.txt file listing all dependencies (LangChain, FAISS, SBERT, Flask).",
    "features": [
      "Bengali Language Support: Complete question-answer system in Bengali.",
      "RAG System: Accurate information retrieval through FAISS vector database.",
      "Modern UI: Beautiful and user-friendly interface built with Tailwind CSS.",
      "Bengali Embeddings: Uses l3cube-pune/bengali-sentence-similarity-sbert model."
    ],
    "link": "https://github.com/debbrath/FAQ_Chatbot"
  }

]
