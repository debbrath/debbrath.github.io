[
  
  {
    "id": "4",
    "categories": ["software", "personal"],
    "image": "./assets/images/project/4.png",
    "title": " ElasticSearch Solutions in .NET CORE",
    "tags": ["Personal","ASP.NET Core",".NET 9","Elasticsearch","Full-Text Search","Data Seeding","Performance Tracking","Entity Framework Core","SQL Server"],
    "objective": "To develop a modern .NET 9 Web API for product management featuring full CRUD operations and dual search functionality using SQL Server and high-performance Elasticsearch.",
    "responsibility":"Implemented full CRUD operations using ASP.NET Core and Entity Framework Core (SQL Server); integrated Elasticsearch via NEST for full-text search; implemented automatic data seeding of 200,000+ products (Bogus); and added performance monitoring middleware and Swagger documentation.",
    "technology":"ASP.NET Core (.NET 9), Entity Framework Core, SQL Server, Elasticsearch, NEST, Bogus, Swagger/OpenAPI.",
    "outcome":"Created a high-performance product API that significantly improved query speed through Elasticsearch integration, ensuring a scalable system with integrated performance tracking and comprehensive API documentation for external developers.",
    "features": [
      ".NET 9 Web API with comprehensive CRUD endpoints.",
      "Dual Search Functionality using SQL Server and Elasticsearch.",
      "Automatic Data Seeding (200,000+ products via Bogus).",
      "Performance Tracking middleware and Swagger documentation."
    ],
    "link": "https://github.com/debbrath/ElasticPracticeSolution"
  },
  {
    "id": "5",
    "categories": ["software", "personal"],
    "image": "./assets/images/project/5.png",
    "title": "eCommerce Application using Microservice Architecture",
    "tags": ["Personal",".NET",".NET Core","Clean Architecture","MongoDB","Redis","PostgreSQL","Ocelot","RabbitMQ","gRPC","Microservices Architecture"],
    "objective": "To build a resilient microservices application using Clean Architecture, integrating polyglot persistence (NoSQL/Relational) and advanced communication (RabbitMQ, gRPC).",
    "responsibility":"Developed key services (Catalog.API with MongoDB, Basket.API with Redis, Discount.API/Grpc with PostgreSQL/Dapper); implemented event-driven communication via RabbitMQ; configured the Ocelot API Gateway for routing; and implemented a robust security stack (Ping Federate, oAuth/JWT).",
    "technology":"ASP.NET, C#, MongoDB, Redis, PostgreSQL, RabbitMQ, Ocelot API Gateway, gRPC, Dapper, Ping Federate, oAuth, JWT, Clean Architecture.",
    "outcome":"Developed a highly scalable and maintainable microservices infrastructure demonstrating expertise in managing complex data structures, high-performance inter-service communication, and enterprise-level security protocols.",
    "features": [
      "Multi-Database Architecture (NoSQL, Relational, Caching).",
      "RabbitMQ for Asynchronous Communication and Ocelot API Gateway.",
      "gRPC (Google Remote Procedure Call) for high-performance service calls.",
      "Clean Architecture principle implemented with C#/.NET."
    ],
    "link": "https://github.com/debbrath/Microservices_Ecommerce"
  },
  {
    "id": "6",
    "categories": ["software", "official"],
    "image": "./assets/images/project/6.png",
    "title": "Employee Assessment System",
    "tags": ["ORION","ASP.Net","MSSQL Server","Role-based KPIs","Job Description Metrics","Functional Performance Assessment","Evaluation","Job-based Performance Indicators (PIs)"],
    "objective": "To build a system that ensures absolute clarity in HR performance evaluation by linking assessment directly to specific job description responsibilities and expected outcomes.",
    "responsibility":"Designed the logic to define and assign unique Performance Indicators (PIs) for each job title; implemented tracking against role-based targets; developed automated performance scoring based on function weightings; and generated cross-role comparison reports.",
    "technology":"ASP.NET (Front and Back End), .NET Framework (4.0), Microsoft SQL Server, Store Procedure, Web Technologies, Crystal Report, Bitbucket/SVN (Source Control), IIS, Windows Server",
    "outcome":"Created a highly transparent and objective appraisal system that successfully drives role-specific excellence by measuring employees against clear, relevant, and fair benchmarks, significantly streamlining the HR appraisal process.",
    "features": [
      "Unique PIs defined and assigned for every specific job title.",
      "Performance metrics linked directly to core duties in job descriptions.",
      "Automated performance scoring based on job function weightings.",
      "Reporting features to compare performance across similar roles."
    ],
    "link": "https://www.linkedin.com/in/debbrathdebnath/"
  },
  {
    "id": "9",
    "categories": ["software", "official"],
    "image": "./assets/images/project/9.png",
    "title": "E-Document Tracking System",
    "tags": ["ORION","Android","Java",".Net Core API","HR & Employee Self-Service (ESS)","Smart Attendance Tracking","Mobile Leave Management","Payslip/Performance App"],
    "objective": "To develop a secure, intelligent, and scalable web-based Document Management System (DMS) to manage the complete document lifecycle from creation to archival.",
    "responsibility":"Established a Centralized Document Repository with advanced indexing; implemented a multi-level Role-Based Approval System with digital signatures; developed Workflow Automation for routing; implemented stringent Version Control and Audit Trails; and ensured Security & Compliance (encryption, GDPR/HIPAA).",
    "technology":"ASP.NET (Front and Back End), .NET Framework (4.5), Microsoft SQL Server, Web Technologies, Crystal Report, Github (Source Control), ERP/HRMS/CRM Integration APIs.",
    "outcome":"Delivered a comprehensive solution that significantly enhanced document security and compliance, improved productivity through workflow automation, and provided reliable audit-readiness while reducing paperwork.",
    "features": [
      "Centralized Repository with Store and organize files in secure, advanced search and metadata indexing.",
      "Role-Based Approval System with digital signatures and audit logs.",
      "Seamless Collaboration with Share documents, add comments, and work together in real time.",
      "Workflow Automation for routing, reminders, and escalation tracking.",
      "Version Control & Audit Trail with Track revisions, maintain history, and ensure compliance."
    ],
    "link": "https://www.linkedin.com/in/debbrathdebnath/"
  },
  {
    "id": "13",
    "categories": ["software", "official"],
    "image": "./assets/images/project/13.png",
    "title": "SAP | SD Module | OHAL | POWER",
    "tags": ["ORION","SAP","SD Module","OHAL","POWER","Data Preparation","User Training","Implementation","SAP Sales Distribution (SD)","Procurement to Pay","Master Data Preparation","Inventory Control","Stock Accounting"],
    "objective": "This comprehensive program provides an in-depth exploration of the Sales Distribution (SD) module, focusing on the entire Procurement to Pay (P2P) cycle within the broader SAP landscape. It covers core MM Business Concepts and the essential preparation of Master Data (Material, Vendor, Service).",
    "responsibility":"Executed all steps in the P2P cycle, including Master Data Preparation (Material, Vendor); processing Purchase Requisition, Quotation, and various Purchasing types; managed Goods Receipt and Incoming Quality Control; and finalized the process with Inventory Control and Vendor Bill Settlement.",
    "technology":"SAP ERP, SAP Sales Distribution (SD) Module.",
    "outcome":"Achieved comprehensive proficiency in managing the end-to-end P2P lifecycle within SAP, enabling streamlined supply chain operations, accurate inventory management, and effective vendor bill settlement processes.",
    "features": [
      "To master and apply the core concepts and processes of the SAP Sales Distribution (SD) module, focusing on the entire Procurement to Pay (P2P) lifecycle.",
      "Focus on Master Data Preparation (Material, Vendor, Service) and Business Concepts.",
      "Covers all core transactions: National Target, Sales, Distribution, Goods Receipt, and Bill Settlement.",
      "Includes advanced topics like Inventory Control, Stock Accounting, and Material Loan Process."
    ],
    "link": "https://www.linkedin.com/in/debbrathdebnath/"
  },
  {
    "id": "14",
    "categories": ["software", "official"],
    "image": "./assets/images/project/14.png",
    "title": "Human Resource Management Information System",
    "tags": ["ORION","ASP.Net","MSSQL Server","HRMIS"],
    "objective": "To develop a comprehensive digital platform that integrates and manages the end-to-end HR Payroll (HR, Payrool, Appraisal) with high visibility and control.",
    "responsibility":"Developed HR Management workflows; implemented Recruitment Management (centralized HR, automated Salary); integrated Financial Controls using a Three-Way Match system.",
    "technology":"ASP.NET (Front and Back End), .NET Framework (4.0, 4.5), Microsoft SQL Server, Web Technologies, Crystal Report, Github (Source Control), Logistics APIs.",
    "outcome":"Delivered an integrated HRMIS that achieved end-to-end visibility, realized significant cost optimization through smarter planning and three-way matching, and enhanced overall regulatory compliance and operational efficiency.",
    "features": [
      "End-to-End Supply Chain Integration (Procurement, Inventory, Logistics).",
      "Automated Stock Replenishment and Batch/Lot Tracking.",
      "Logistics Optimization with route planning and delivery scheduling.",
      "Financial Control via Three-Way Match and automated MRP."
    ],
    "link": "https://www.linkedin.com/in/debbrathdebnath/"
  },
  {
    "id": "51",
    "categories": ["ai-solutions", "personal"],
    "image": "./assets/images/project/51.png",
    "title": "Diabetes Detection System",
    "tags": ["Personal","Binary Classification","Diabetes Prediction","Logistic Regression","KNN","Decision Tree","Random Forest","Stochastic Gradient Descent","Gradient Boosting","AdaBoost","Extra Trees","Support Vector Machine","Naive Bayes","ROC AUC","Feature Scaling","Machine Learning"],
    "objective": "To develop a robust binary classification model capable of predicting the presence or absence of diabetes based on a medical features dataset, with a focus on implementing and comparatively evaluating multiple machine learning classifiers.",
    "responsibility":"Split the dataset into training and testing sets; applied feature scaling (e.g., using StandardScaler); implemented and trained at least three distinct classifiers (Logistic Regression, KNN, Decision Tree); rigorously evaluated and compared each model's performance; and generated visualizations including the ROC Curve and Confusion Matrix.",
    "technology":"Python, Machine Learning Library (Scikit-learn implied), Pandas, Matplotlib/Seaborn, Logistic Regression, K-Nearest Neighbors (KNN), Decision Tree, StandardScaler.",
    "outcome":"Successfully identified the optimal model for diabetes prediction (based on the highest ROC AUC and F1-score), demonstrating proficiency in classification workflows, comparative analysis, and advanced performance evaluation crucial for healthcare applications.",
    "features": [
      "Binary Classification: Predicts diabetes status (0 or 1) from medical features.",
      "Model Comparison: Evaluates performance across Logistic Regression, KNN, and Decision Trees.",
      "Essential Preprocessing: Mandatory use of Feature Scaling to normalize input data ranges.",
      "Rigorous Evaluation: Uses Confusion Matrix, P/R/F1-score, and ROC AUC for performance assessment."
    ],
    "link": "https://colab.research.google.com/drive/1vxwTmYmujdCPBt8_KrrqSVBCPB3qgc8f"
  },
  {
    "id": "52",
    "categories": ["ai-solutions", "personal"],
    "image": "./assets/images/project/52.png",
    "title": "Clustering Assignment using Iris Dataset",
    "tags": ["Personal","Clustering Algorithms","K-Means","DBSCAN","Hierarchical","Iris Dataset","StandardScaler","Unsupervised Learning"],
    "objective": "To apply and compare three distinct unsupervised clustering algorithms (K-Means, Hierarchical, DBSCAN) to group the flowers of the Iris dataset, solely using the four continuous features, to understand their effectiveness in finding natural groupings.",
    "responsibility":"Performed Data Preprocessing (feature scaling using StandardScaler); conducted Exploratory Data Analysis (EDA); systematically applied K-Means (using the Elbow Method to find optimal k), Hierarchical Clustering (using a Dendrogram), and DBSCAN (using a k-distance graph to find eps); and visualized the resulting clusters (e.g., via PCA).",
    "technology":"Scikit-learn, Pandas, Matplotlib/Seaborn (for EDA and visualization), StandardScaler, PCA (for visualization).",
    "outcome":"Successfully performed a comparative analysis of partition-based, density-based, and hierarchical clustering techniques. The project demonstrates proficiency in unsupervised learning methodology, optimal parameter selection, and the use of visualization to interpret cluster quality and structure.",
    "features": [
      "Comparative Clustering: Directly compares K-Means, Hierarchical, and DBSCAN algorithms.",
      "Unsupervised Preprocessing: Applied to the Iris Dataset using StandardScaler for feature scaling.",
      "Parameter Optimization: Uses the Elbow Method (K-Means) and Dendrogram (Hierarchical) to select optimal parameters.",
      "Insight Generation: Includes EDA and Visualization to assess the final cluster groupings."
    ],
    "link": "https://colab.research.google.com/drive/1HfyT-qS-fBa88ZiRuFPx1hM6D1PVSD4h"
  },
  {
    "id": "53",
    "categories": ["ai-solutions", "personal"],
    "image": "./assets/images/project/53.png",
    "title": "Digit Classifier (MNIST)",
    "tags": ["Personal","MNIST Classifier","Batch Normalization","Dropout Regularization","Keras","Deep Learning","Deep Learning Architecture"],
    "objective": "To build and evaluate a Deep Neural Network (DNN) for classifying handwritten digits using the MNIST dataset, with a core focus on reinforcing essential deep learning concepts like data preprocessing and regularization techniques.",
    "responsibility":"Performed necessary data preprocessing (Normalization and Flattening of 28x28 images); designed the DNN architecture incorporating Batch Normalization (for faster convergence) and Dropout layers (to mitigate overfitting); compiled and trained the model (e.g., using categorical_crossentropy and adam); and generated visual plots comparing training and validation accuracy/loss over epochs.",
    "technology":"Deep Learning Framework (TensorFlow/Keras, implied), MNIST Dataset, Python, Dense Layers, Batch Normalization, Dropout Layers.",
    "outcome":"Successfully built and optimized a high-accuracy DNN for digit classification, demonstrating practical application of key deep learning regularization techniques, resulting in a reliable model and clear visualization of its learning behavior.",
    "features": [
      "DNN Architecture: Built a Deep Neural Network for the Handwritten Digit Classification task.",
      "Essential Regularization: Mandatory use of Batch Normalization and Dropout layers for model stability.",
      "Data Preparation: Includes essential preprocessing steps like Normalization and Flattening the input data.",
      "Performance Reporting: Visual comparison of Training vs. Validation Accuracy and final test accuracy report."
    ],
    "link": "https://github.com/debbrath/Digit-Classifier-MNIST"
  },
  {
    "id": "54",
    "categories": ["ai-solutions", "personal"],
    "image": "./assets/images/project/54.png",
    "title": "Image Classification with NN vs CNN",
    "tags": ["Personal","Image Classification","NN","CNN","NN vs CNN","Fashion MNIST","TensorFlow","Flask","FastAPI","Deep Learning","Deep Learning Architecture"],
    "objective": "To conduct a comparative performance analysis between a standard Neural Network (NN) and a Convolutional Neural Network (CNN) on the Fashion MNIST dataset, explicitly highlighting the architectural superiority of CNNs for visual classification tasks.",
    "responsibility":"Preprocessed the 28x28 grayscale images (normalization, flattening for NN, and reshaping for CNN); designed and built both the fully-connected NN and the CNN (using Conv2D and MaxPooling layers); trained both models on the dataset; and generated final outputs including test accuracy and visualized sample predictions for a quantified comparison.",
    "technology":"Deep Learning Framework (TensorFlow/Keras, implied), Python, Fashion MNIST Dataset, Dense Layers, Conv2D Layers, MaxPooling Layers.",
    "outcome":"Provided empirical evidence demonstrating the significant performance advantage of feature extraction via CNN layers over dense layers for image data, reinforcing core concepts in Deep Learning Architecture and improving model accuracy for garment classification.",
    "features": [
      "Comparative Model Study: Directly contrasts a basic NN against a powerful CNN architecture.",
      "Fashion MNIST Classification: Solves a 10-category image classification problem.",
      "Data Preparation: Essential preprocessing includes normalization and model-specific reshaping (flattening vs. 28x28x1).",
      "Quantified Comparison: Displays test accuracy and visualized sample predictions to prove CNN efficacy."
    ],
    "link": "https://github.com/debbrath/ImageClassification"
  },
  {
    "id": "55",
    "categories": ["ai-solutions", "personal"],
    "image": "./assets/images/project/55.png",
    "title": "Object Tracking & Heatmap Visualization",
    "tags": ["Personal","People Flow Detection","Object Tracking (ByteTrack/SORT)","ByteTrack","SORT","YOLO Detection","Heatmap Visualization","Counting"],
    "objective": "To build a Computer Vision system that integrates object tracking, motion counting, and heatmap generation to monitor and quantify the flow (IN/OUT) of people across a defined area in a video feed.",
    "responsibility":"Implemented a YOLO model for robust person detection; integrated a high-performance Object Tracker (e.g., ByteTrack) to assign and maintain unique person IDs; developed custom logic to define and monitor movement across two horizontal lines; calculated the IN/OUT directional counts; and generated a final Heatmap visualizing areas of high presence/motion intensity.",
    "technology":"YOLO (for detection), ByteTrack / DeepSORT / SORT (for tracking), Python, OpenCV (for video/visuals), Roboflow PolygonZone (for defining lines).",
    "outcome":"Created an automated system for retail analytics, security, or traffic monitoring that provides real-time, quantitative metrics (IN/OUT counts) and spatial insights (heatmap) into human movement patterns, enhancing operational and security assessments.",
    "features": [
      "Object Detection and Tracking: Uses YOLO for detection and a high-performance tracker (ByteTrack/DeepSORT) to maintain unique IDs.",
      "Directional Flow Counting: Implements specific IN/OUT logic based on crossing two horizontal lines with defined directionality.",
      "Spatial Visualization: Generates a Heatmap to visualize areas of highest person presence or motion intensity.",
      "Real-time Output: Provides live counters and bounding boxes with unique IDs overlaid on the video."
    ],
    "link": "https://colab.research.google.com/drive/1y_yhGvKvHJdmT7mygEK7I2Ii0R9NJ6mj?usp=drive_open"
  },
  {
    "id": "56",
    "categories": ["ai-solutions", "personal"],
    "image": "./assets/images/project/56.png",
    "title": "Model & APIs Deployment on Render",
    "tags": ["Personal","FastAPI Deployment","Docker","Machine Learning API","Render Cloud Host","Model","Deployement"],
    "objective": "To build, containerize (Docker), and deploy a FastAPI web service serving predictions from a machine learning classifier for Heart Disease detection.",
    "responsibility":"Trained and persisted a basic machine learning model (e.g., Logistic Regression); developed the FastAPI application with Pydantic schemas and necessary endpoints (/predict, /health); wrote the Dockerfile and docker-compose.yml; and successfully deployed the containerized service to a cloud host (Render).",
    "technology":"FastAPI, Pydantic, Docker, Docker-Compose, Joblib, Scikit-learn, Render (Cloud Host).",
    "outcome":"Successfully achieved a continuous deployment workflow for an ML model, demonstrating proficiency in MLOps by creating a scalable, production-ready microservice accessible via a well-documented REST API.",
    "features": [
      "FastAPI serving predictions from a Heart Disease Classifier.",
      "Pydantic schema validation for all input features.",
      "Dockerization for consistent, reproducible environments.",
      "Cloud Deployment (e.g., Render) of the containerized application."
    ],
    "link": "https://github.com/debbrath/DiabetesPrediction"
  },
  {
    "id": "57",
    "categories": ["ai-solutions", "personal"],
    "image": "./assets/images/project/57.png",
    "title": "Sentiment Analysis using IMDB Dataset",
    "tags": ["Personal","Sentiment Analysis","Text Vectorization","IMDB","TF–IDF","Word2Vec","BERT","NLP Classification"],
    "objective": "To compare the performance of sentiment classification models on the IMDB dataset using three different text representation (vectorization) methods.",
    "responsibility":"Loaded and preprocessed the IMDB dataset; implemented and trained separate classifiers using TF-IDF, Word2Vec embeddings, and BERT embeddings; evaluated all models on the test set; and created a comparative report detailing performance trade-offs (accuracy, time, resources).",
    "technology":"TF-IDF, Word2Vec, BERT embeddings, Classification Models (e.g., Logistic Regression), Hugging Face/TensorFlow Datasets, Google Colab.",
    "outcome":"Conducted a comprehensive analysis that provided data-driven insights into how different NLP feature engineering approaches impact classification accuracy, memory usage, and training time.",
    "features": [
      "Sentiment Analysis on the IMDB Reviews Dataset.",
      "Comparative study of TF–IDF, Word2Vec, and BERT embeddings.",
      "Evaluation using Accuracy, Precision, Recall, and F1 Score.",
      "Focus on Text Representation impact on classification performance."
    ],
    "link": "https://github.com/debbrath/Sentiment-Analysis"
  },
  {
    "id": "58",
    "categories": ["ai-solutions", "personal", "official"],
    "image": "./assets/images/project/58.png",
    "title": "Fine Tuning Transformers for Question Answer",
    "tags": ["Transformer Fine-Tuning","Question Answering (QA)","Hugging Face","SQuAD","BERT","Exact Match (EM)"],
    "objective": "To fine-tune a pre-trained BERT-based model on the SQuAD v1.1 dataset to perform the Question Answering (QA) task, demonstrating proficiency in the Hugging Face ecosystem.",
    "responsibility":"Loaded and preprocessed the SQuAD dataset (tokenization, answer mapping); loaded the bert-base-uncased model; fine-tuned the model using the Hugging Face Trainer API; and evaluated performance using Exact Match (EM) and F1 Score metrics.",
    "technology":"Hugging Face transformers, datasets, BERT-base-uncased, SQuAD v1.1, Google Colab.",
    "outcome":"Successfully trained a state-of-the-art model for extractive QA, achieving measurable performance and demonstrating core competencies in advanced deep learning model fine-tuning and evaluation for NLP.",
    "features": [
      "Transformer Fine-Tuning for Question Answering (QA).",
      "Uses BERT-base-uncased on the SQuAD v1.1 dataset.",
      "Implementation via the Hugging Face datasets and transformers APIs.",
      "Evaluation using Exact Match (EM) and F1 Score metrics."
    ],
    "link": "https://colab.research.google.com/drive/1aj4-juMwg28_ZKPEZFB_-22XXdHLAW1R?usp=drive_open"
  },
  {
    "id": "59",
    "categories": ["ai-solutions", "personal"],
    "image": "./assets/images/project/59.png",
    "title": "AI Agent for LinkedIn Post Generator",
    "tags": ["Personal","LangChain Agent","LinkedIn Post Generator","LLM Chain","Content Generation","Multilingual AI","GitHub Models", "Lovable"],
    "objective": "To develop a LangChain AI Agent that generates professional, structured LinkedIn posts based on a user-provided topic and language.",
    "responsibility":"Designed and built the AI Agent using LangChain; configured the Large Language Model (LLM) chain to accept and process multilingual inputs; ensured the generated output was an engaging, platform-specific post (2–4 paragraphs).",
    "technology":"LangChain, LLM (Large Language Model), AI Agent.",
    "outcome":"Developed a scalable content creation tool that automates the generation of multilingual, professional social media content, ensuring platform-specific quality and increasing user productivity.",
    "features": [
      "LangChain AI Agent for content generation.",
      "Multilingual Post Creation based on user-specified language.",
      "LinkedIn-Optimized Output (professional, engaging structure).",
      "Input variables: Topic and Language."
    ],
    "link": "https://github.com/debbrath/PostGenerator-AIAgent-LangChain"
  },
  {
    "id": "60",
    "categories": ["ai-solutions", "personal"],
    "image": "./assets/images/project/60.png",
    "title": "AI Agent Project with n8n",
    "tags": ["Personal","n8n Workflow Automation","AI Article Summarization","FastAPI & Webhooks","No-Code","Low-Code AI","Gemini Models", "Lovable"],
    "objective": "To build a full-stack, automated AI workflow system using n8n to process article URLs submitted via a Frontend/FastAPI Backend.",
    "responsibility":"Configured the FastAPI backend to send user input to an n8n webhook; developed the core n8n workflow for web scraping (Firecrawl), generating AI summaries and key insights (OpenAI); automated data persistence to Google Sheets; and implemented the final email delivery of results to the user.",
    "technology":"n8n Workflow Automation, FastAPI, Webhooks, Firecrawl, OpenAI Node, Google Sheets, Email Tool.",
    "outcome":"Delivered an automated, low-code solution that drastically reduced manual article analysis time by instantaneously scraping, processing, analyzing, archiving, and communicating results to the end-user.",
    "features": [
      "Full-Stack Workflow: Frontend, FastAPI Backend, and n8n orchestration.",
      "AI-Powered Article Analysis: Summarization and key insight extraction.",
      "Automated Data Flow: Webhook trigger, data persistence to Google Sheets, and user email delivery.",
      "n8n Automation: Used for all core intelligence and tool integration (Firecrawl, OpenAI, Google Sheets, Email)."
    ],
    "link": "https://github.com/debbrath/AI-Agent-N8N"
  },
  {
    "id": "61",
    "categories": ["ai-solutions", "personal", "official"],
    "image": "./assets/images/project/61.png",
    "title": "Multitools Agent Medical Queries",
    "tags": ["Multitools AI Agent","Agent Routing","SQLite","LangChain", "GitHub Models", "Streamlit"],
    "objective": "To build a Multi-Tool AI Agent capable of answering data-specific queries from medical datasets and retrieving general medical knowledge using web search.",
    "responsibility":"Built the main AI Agent logic for intelligent query routing; converted three medical datasets (Heart Disease, Cancer, Diabetes) into dedicated SQLite DBs; developed three DB-specific Langchain Agents (Tools); and integrated a Medical Web Search Tool.",
    "technology":"OpenAI Agent SDK, Langchain Agent Executor, SQLite, SQL, Web Search API (SerpAPI/Tavily/Bing).",
    "outcome":"Created an intelligent system that unified access to both structured data (for statistics) and general knowledge (for definitions), delivering complex medical query results in a single, natural language response.",
    "features": [
      "Intelligent Query Routing to select the best knowledge source.",
      "Three Dedicated Database Tools for querying medical statistics via SQL.",
      "Integration of a Web Search Tool for general, up-to-date medical knowledge.",
      "Uses OpenAI Agent SDK and Langchain Agent Executor for robust logic.",
      "Mixed Queries → Both database and web search tools"
    ],
    "link": "https://github.com/debbrath/Multi-Tool-Medical-AIAgent"
  },
  {
    "id": "62",
    "categories": ["ai-solutions", "personal"],
    "image": "./assets/images/project/62.png",
    "title": "CrewAI for Instagram Content Creation",
    "tags": ["Personal","CrewAI","CrewAI Tools","LangChain Community", "GitHub Models", "Multi-Agent","Multi-Agent System", "Image Generation", "Instagram Content Creation"],
    "objective": "To design and implement a Multi-Agent System using Crew AI to fully automate the end-to-end pipeline for Instagram content creation, from initial topic research to final image generation.",
    "responsibility":"Designed and orchestrated a 4-agent team: Research Agent (information gathering), Content Writer Agent (caption/CTA drafting), Reviewer Agent (editing/approval), and Image Prompt Generator Agent (detailed text-to-image prompts); integrated agents using the Crew AI framework to ensure sequential and collaborative workflow; and connected to an external text-to-image API for final visual asset creation.",
    "technology":"Crew AI (Framework), LLM (e.g., OpenAI/Anthropic/other models for agents), External Text-to-Image API (Nano Banana, Segmind, Stable Diffusion, etc.), Python, Research Tools (Web search/scrapers accessible by the Research Agent).",
    "outcome":"Delivered a complete, automated content package (caption, hashtags, 2-3 images) for any given topic. The system demonstrates advanced AI workflow automation, achieving efficiency and consistency in digital marketing content creation by mimicking a human team structure.",
    "features": [
      "4-Agent Orchestration: Implements a collaborative team (Researcher, Writer, Reviewer, Prompt Generator).",
      "Crew AI Framework: Used to define roles, goals, and manage the sequential workflow.",
      "Full-Cycle Automation: Automates content generation from research/writing to final image creation.",
      "External API Integration: Connects the text-based workflow to an image generation service for visual output."
    ],
    "link": "https://github.com/debbrath/crewai-multiagent-instagram-pipeline"
  }  ,
  {
    "id": "63",
    "categories": ["ai-solutions", "personal", "official"],
    "image": "./assets/images/project/63.png",
    "title": "Bangla FAQ Chatbot",
    "tags": ["Bangla","Bangla FAQ","FAQ Chatbot", "RAG", "HuggingFace Model", "GitHub Models", "Multi-Agent"],
    "objective": "Develop an interactive Bengali FAQ Chatbot using the RAG architecture to demonstrate proficiency in core AI and web integration technologies. Key requirements include: Bengali NLP, FAISS vector retrieval with metadata filtering, and full-stack integration using Flask/Gemini.",
    "responsibility":"Create a Bengali FAQ dataset (5 topics, ≥3 FAQs each), tag with metadata (topic/difficulty), and initialize FAISS using the Bengali SBERT model. Implement a Flask server to manage user sessions. Execute FAISS similarity search incorporating dynamic metadata filtering. Use the retrieved context to generate answers via the Gemini API. Build a responsive UI (index.html) using Tailwind CSS. Ensure interactive button-based menus for topic and difficulty filtering, and implement a Bengali fallback mechanism.",
    "technology":"RAG, FAQ Chatbot, HuggingFace Model, GitHub Model, Python.",
    "outcome":"A functional RAG web application that answers Bengali queries based on the filtered FAQ data. A clean, modular Python codebase (app.py, rag_core.py) demonstrating advanced RAG techniques. A comprehensive requirements.txt file listing all dependencies (LangChain, FAISS, SBERT, Flask).",
    "features": [
      "Bengali Language Support: Complete question-answer system in Bengali.",
      "RAG System: Accurate information retrieval through FAISS vector database.",
      "Modern UI: Beautiful and user-friendly interface built with Tailwind CSS.",
      "Bengali Embeddings: Uses l3cube-pune/bengali-sentence-similarity-sbert model."
    ],
    "link": "https://github.com/debbrath/FAQ_Chatbot"
  }

]
